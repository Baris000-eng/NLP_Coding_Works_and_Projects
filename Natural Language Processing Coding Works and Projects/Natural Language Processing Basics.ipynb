{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a354aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Spacy?\n",
    "# * It is an open-source natural language processing library.\n",
    "# * It is designed to effectively handle NLP tasks with the \n",
    "# most efficient implementation of common algorithms.\n",
    "# * For many NLP tasks, Spacy only has one implemented method,\n",
    "# which chooses the most efficient algorithm currently available.\n",
    "\n",
    "# What is NLTK?\n",
    "# * NLTK is an open-source NLP library, namely Natural Language Toolkit.\n",
    "# * It is firstly released in 2001, whereas the Spacy is firstly released in 2015.\n",
    "# * It includes less efficient implementations than Spacy.\n",
    "\n",
    "# NLTK vs Spacy\n",
    "# * For many common NLP tasks, Spacy is much faster and more efficient.\n",
    "# However, this comes with the cost of single choice for each available \n",
    "# algorithm.\n",
    "# * However, Spacy does not include pre-created models for some applications, such as\n",
    "# sentiment analysis, which is typically easier to perform with NLTK.\n",
    "# * https://spacy.io/usage/facts-figures (for having more information about the comparisons of NLP libraries \n",
    "# such as NLTK, Spacy, and CoreNLP)\n",
    "\n",
    "# What is Natural Language Processing (NLP)?\n",
    "\n",
    "# Natural Language Processing (NLP) is an area of computer science and artificial intelligence\n",
    "# concerned with the interactions between computers and human (natural) languages, in particular\n",
    "# how to program computers to process and analyze large amounts of natural language data.\n",
    "\n",
    "# Natural Language Processing attempts to use a variety of techniques in order to \n",
    "# create structure out of text data.\n",
    "\n",
    "# Text data is highly unstructured and can be in multiple languages.\n",
    "\n",
    "# Some example use cases of NLP:\n",
    "# Classifying Emails as Spam vs Legitimate\n",
    "# Sentiment Analysis of Movie Reviews' Texts\n",
    "# Analyzing trends from written customer feedback forms.\n",
    "# Understanding text commands like \"Hey Siri, play this song\".\n",
    "\n",
    "# Spacy Basics:\n",
    "# * Loading the language library\n",
    "# * Building a pipeline object\n",
    "# * Using tokens\n",
    "# * Parts-of-Speech Tagging\n",
    "# * Understanding Token Attributes\n",
    "\n",
    "\n",
    "# Spacy works with a pipeline object. \n",
    "# The nlp() function from Spacy automatically takes a raw text and performs a series of operations to \n",
    "# tag, parse, and desribe the text data. These operations include tokenization, parsing, named entity \n",
    "# recognition, and so forth.\n",
    "# Tokenization \n",
    "\n",
    "\n",
    "\n",
    "# Stemming\n",
    "\n",
    "\n",
    "\n",
    "# Lemmatization\n",
    "\n",
    "\n",
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf6eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For downloading the core english language library, we can use below command.\n",
    "python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "201ea043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.2.0) (3.2.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.10)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0)\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-macosx_10_9_x86_64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (61.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (23.1)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.7.4.1 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.5.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/barissss/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\n",
      "Installing collected packages: pydantic, en-core-web-sm\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.2\n",
      "    Uninstalling pydantic-1.10.2:\n",
      "      Successfully uninstalled pydantic-1.10.2\n",
      "Successfully installed en-core-web-sm-3.2.0 pydantic-1.8.2\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# For downloading the small core english language library, we can use below command.\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d973ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x7f88a53a38e0>\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "926c206a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla is looking at buying U.S. startup for $10 million dollars.\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "\n",
      "\n",
      "\n",
      "Tesla Tesla 96 PROPN nsubj\n",
      "is is 87 AUX aux\n",
      "looking looking 100 VERB ROOT\n",
      "at at 85 ADP prep\n",
      "buying buying 100 VERB pcomp\n",
      "U.S. U.S. 96 PROPN dobj\n",
      "startup startup 100 VERB dep\n",
      "for for 85 ADP prep\n",
      "$ $ 99 SYM quantmod\n",
      "10 10 93 NUM compound\n",
      "million million 93 NUM nummod\n",
      "dollars dollars 92 NOUN pobj\n",
      ". . 97 PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "document = nlp(u'Tesla is looking at buying U.S. startup for $10 million dollars.')\n",
    "# document object holds the processed text.\n",
    "print(document)\n",
    "print(type(document))\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "# Parts of Speech: \n",
    "# PROPN = Proper Noun\n",
    "# VERB = Verb\n",
    "# SYM = Symbol\n",
    "# NUM = Number\n",
    "# PUNCT = Punctuation\n",
    "# NOUN = Noun\n",
    "# ADP = Adposition\n",
    "# AUX = Auxillary Verb\n",
    "\n",
    "# printing the token, text of the token, the part of speech, and the syntactic dependency of the token.\n",
    "for token in document:\n",
    "    print(token, token.text, token.pos, token.pos_, token.dep_)\n",
    "    \n",
    "    \n",
    "    # token.pos_ command will get the raw name of part of speech \n",
    "    # for each token in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df76e4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7f853847a520>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7f853847a7c0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7f8535eb7f90>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7f853a50dbc0>),\n",
       " ('lemmatizer',\n",
       "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7f853a516300>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7f8545e75740>)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline # The NLP pipeline for the nlp object named 'nlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3718f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f853847a520>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f853847a7c0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f8535eb7f90>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f853a50dbc0>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7f853a516300>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f8545e75740>)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19b17edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "pipes = nlp.pipe_names #to get the basic names of the subparts of the NLP pipeline\n",
    "print(pipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c03f51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "looking VERB ROOT\n",
      "into ADP prep\n",
      "startups NOUN compound\n",
      "anyore NOUN pobj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "second_document = nlp(u\"Tesla isn't looking into startups anyore.\")\n",
    "\n",
    "# For the word \"isn't\", Spacy is able to recognize both the root word and the negation attached to it.\n",
    "for token in second_document:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b12d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
