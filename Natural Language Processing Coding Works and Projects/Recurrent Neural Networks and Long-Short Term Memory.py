# -*- coding: utf-8 -*-
"""RNNs and LSTMs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12xy7otZ97zaPmxtzvzb9r2BmKEwxEjD4
"""

# Recurrent Neural Networks (RNNs)

# Recurrent neural networks are specifically designed to work with sequence data.
# Examples of sequences:
# 1-) Time series data (E.g. Sales)
# 2-) Audio
# 3-) Sentences
# 4-) Car Trajectories (Sequence of instructions: left, right, forward, and back)
# 5-) Music

# Normal Neuron in Feed Forward Network:

# It takes some input, aggregates them if there are multiple inputs, passes it/them
# through the activation function, and obtain an output.
# Here, the input can consist of a single input or multiple inputs.

# Recurrent Neuron:

# * It sends the output back to itself!
# * Cells that are a function of inputs from previous time stamps are also known as
# memory cells.
# * RNNs are also flexible in their inputs and outputs for both sequences and single
# vector values.
# * It is very easy to create an entire layer of recurrent neurons.
# * Part of the recurrent neural network which preserves some sort of state across
# the time stamps is called a 'memory cell'.
# * In the recurrent neurons, outputs are sent back into the inputs.
# * Recurrent Neural Networks (RNNs) are very flexible in their inputs and outputs.

# * Sequence to Sequence (Example: Passing in a set of time series information, such as a year's worth of montly
# sales data, and then wanting back a sequence of that same sales data shifted over certain timeperiod into the feature.)
# * Sequence to Vector (Example: Sentiment scores)
# - We can feed in a sequence of words and request back a vector which indicates whether it was a positive or
# negative sentiment.
# * Vector to Sequence (Example: Providing a single seed word and then getting out an entire sequence of high-probability
# sequence phrases.)

# Long Short Term Memory (LSTM) and GRU
# An issue RNN faces is that after awhile the network will begin to 'forget' the first inputs, as information is lost
# at each step going through the RNN (especially we train the network on a really large sequence). Therefore, we need
# some sort of 'long-term memory' for our networks. We need to balance both the short term memory of the networks, the
# data that it was recently trained on, versus the long term memory of the networks, train data starting from the very
# first and ending at the most recent.

# The LSTM (Long Short Term Memory) cell was created to help addressing these RNN issues.

# In a typical RNN cell, the output at time t-1 is fed along with the input at time t.

# In an LSTM, the very first step is called 'forget gate layer'. In this step, we decide what information are we going
# to forget from the cell state.

# An LSTM Cell

# f(t) = sigma * (wf * [h(t-1), x(t)] + b(f))

# We pass h(t-1) and x(t) to the f(t), after performing a linear transformation with sigmoid.


# 1 represents 'keeping the information'.
# 0 represents 'forgetting about / getting rid of the information'.

# If we think of a language model where we try to predict predict the next word based on previous ones, a cell state
# might contain the gender of the present subject; therefore, we end up picking the correct pronoun.

#------------------------------------------------------------------------------------------------------------------#

# In the second step, we decide what to keep in the cell state.

# Another variation of the LSTM cell is called 'the gated recurrent unit' or 'GRU'. This was introduced quite recently,
# around 2014. It combines the forget and input gates into a single gate called 'update gate'. It also merges the cell
# state and hidden state.
# GRU is simpler than the LSTM model.

# Depth-gated recurrent neural network was released in 2015.

from google.colab import drive
drive.mount('/content/drive/')

cd 'drive/MyDrive'

# Text generations with LSTMs by using Keras and Python

def read_file(filepath):
  if (filepath is None) or (len(filepath) == 0):
    raise Exception('Invalid filepath is found !')
  with open(filepath) as file:
    text = file.read()

  return text



read_file('moby_dick_four_chapters.txt')

moby_dick_text = read_file('moby_dick_four_chapters.txt')
print(moby_dick_text)

# clean and tokenize the text
import spacy

# loading the small english core language library
nlp = spacy.load('en_core_web_sm', disable = ['parser', 'ner', 'tagger'])
print(nlp)
print(type(nlp))

nlp.max_length = 1200000

def seperate_punctuation(doc_text):
  punctuation = '-/&()%+^!\n?\t.-,--'
  without_punc = []
  for token in nlp(doc_text):
    if token.text not in punctuation:
        text = token.text
        without_punc.append(text.lower())

  return without_punc


file = read_file('moby_dick_four_chapters.txt')
tokens = seperate_punctuation(file)
print(tokens)
print(len(tokens))
print('There are '+str(len(tokens))+' tokens in the moby_dick_four_chapters.txt file.')

from IPython.core.formatters import JavascriptFormatter
# 29 words ----> neural network predicts the word #30

train_len = 30
text_sequences = []
for j in range(train_len, len(tokens)):
  sequence = tokens[j-train_len:j]
  text_sequences.append(sequence)

print(text_sequences)

print(text_sequences[0])

print(text_sequences[1])

print(len(text_sequences))

' '.join(text_sequences[0])

' '.join(text_sequences[1])

from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(text_sequences)

sequences = tokenizer.texts_to_sequences(text_sequences)
print(sequences)

# Each of the numbers is an ID particular to the word.

print(sequences[0])
print()
print()
print(sequences[1])



print()
print(len(sequences))
print(len(sequences[0]))

# This returns a dictionary where unique id values are mapped to different words. In this dictionary, the keys are
# the unique id values. Moreover, the values are different words.
tokenizer.index_word

print(tokenizer.index_word)
print()
print()
for i in sequences[0]:
  print(f'{i}: {tokenizer.index_word[i]}')

tokenizer.word_counts

vocabulary_size = len(tokenizer.word_counts)
print('The vocabulary size: '+str(vocabulary_size)+'')
print(type(vocabulary_size))

import numpy as np

sequences = np.array(sequences)

print(sequences)

sequences

from keras.utils import to_categorical

print(sequences[:,:-1])

# loading the data
X = sequences[:,:-1]
print(len(X))
y = sequences[:, -1]
print(len(y))
y = to_categorical(y, num_classes = vocabulary_size + 1)

print(X.shape, y.shape)

seq_len = X.shape[1]
print(seq_len)

print(X.shape)

from keras.layers import LSTM, Dense, Embedding
from keras.models import Sequential

# The batch size is the number of sequences you
# want to pass.
def create_model(vocabulary_size, seq_len):
    model = Sequential()
    print('The vocabulary size: '+str(vocabulary_size)+'')
    print('The sequence length: '+str(seq_len)+'')
    model.add(Embedding(vocabulary_size, 29, input_length=seq_len))
    model.add(LSTM(150, return_sequences=True))
    model.add(LSTM(150))
    model.add(Dense(150, activation='relu'))

    model.add(Dense(vocabulary_size, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    model.summary()

    return model

model = create_model(vocabulary_size + 1, seq_len)
print(seq_len)

from pickle import load, dump
# Batch size is how many sequences to pass at a time.
# training the neural network model
model.fit(X, y, batch_size = 128, epochs = 20, verbose = 1)

model.save('my_moby_dick_model.h5') # saving the model with the name 'my_moby_dick_model'
dump(tokenizer, open('my_simple_tokenizer', 'wb')) # save the tokenizer file called 'my_simple_tokenizer' in a 'write binary (wb)' mode

# As you can see in the output, the loss and accuracy are inversely proportional to each other.
# Given that the number of epochs increases, while the loss is decreasing, accuracy is increasing.

# Here, the number of epochs is 200 instead of 20.
model.fit(X, y, batch_size = 128, epochs = 200, verbose = 1)

# save the model where the epoch is big to the file
model.save('epoch_big.h5')

# save the tokenizer
with open('epoch_big', 'wb') as epoch_big:
  dump(tokenizer, epoch_big)

import os

# Get the current working directory
current_directory = os.getcwd()

# Print the current directory
print("Current working directory:", current_directory)

from pickle import load
from random import randint
from keras.preprocessing.sequence import pad_sequences
from keras.models import load_model


# Generating new text
def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):
    '''
    INPUTS:
    model : model that was trained on text data
    tokenizer : tokenizer that was fit on text data
    seq_len : length of training sequence
    seed_text : raw string text to serve as the seed
    num_gen_words : number of words to be generated by model
    '''

    # Final Output
    output_text = []

    # Intial Seed Sequence
    input_text = seed_text

    # Create num_gen_words
    for i in range(num_gen_words):

        # Take the input text string and encode it to a sequence
        encoded_text = tokenizer.texts_to_sequences([input_text])[0]

        # Pad sequences to our trained rate (50 words in the video)
        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')

        # Predict class probabilities for each word
        pred_probabilities = model.predict(pad_encoded, verbose=0)[0]

        print('-------------------------------------')
        print(model.predict(pad_encoded, verbose = 0))
        print(pred_probabilities)

        # Find the index of the word with the highest probability
        pred_word_ind = np.argmax(pred_probabilities)
        print(pred_word_ind)
        print('-------------------------------------')

        #predict_x=model.predict(X_test)
        #classes_x=np.argmax(predict_x,axis=1)

        # Extract word
        pred_word = tokenizer.index_word[pred_word_ind]

        # Update the sequence of input text (shifting one over with the new word)
        input_text += ' ' + pred_word

        output_text.append(pred_word)

    # Make it look like a sentence
    return ' '.join(output_text)

text_sequences[0]

import random
random.seed(123) # to get the same randomization in different runs while picking a random seed text
random_pick = random.randint(0,len(text_sequences))

print('The list of text sequences is: ')
print()
print()
print(text_sequences)
print(type(text_sequences))
print('The length of the text_sequences list is: '+str(len(text_sequences))+'')
print('The random pick is: '+str(random_pick)+'')

random_seed_text = text_sequences[random_pick]

random_seed_text

print('The random seed text is as below: ')
print()
print(random_seed_text)

seed_text = ' '.join(random_seed_text)

seed_text

print('The joined seed text is as below: ')
print()
print(seed_text)

generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=100)

full_text = read_file('moby_dick_four_chapters.txt')
for i,word in enumerate(full_text.split()):
    if word == 'inkling':
        print(' '.join(full_text.split()[i-20:i+20]))
        print('\n')

from keras.models import load_model

model = load_model('epoch_big.h5')
tokenizer = load(open('epoch_big', 'rb'))
generate_text(model, tokenizer, seq_len, seed_text=seed_text, num_gen_words = 29)



