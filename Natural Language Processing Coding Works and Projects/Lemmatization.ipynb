{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "721c329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In contrast to stemming, lemmatization looks beyond word reduction and considers a language's full vocabulary \n",
    "# to apply a morphological analysis to words.\n",
    "\n",
    "# The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'.\n",
    "\n",
    "# Lemmatization is much more informative than stemming, which is why the Spacy library has opted to only have \n",
    "# lemmatization available instead of stemming. \n",
    "\n",
    "# Lemmatization looks at surrounding text to determine a given word's part of speech."
    "# Lemmatization is a text pre-processing technique used in natural language processing (NLP) models 
# to break a word down to its root meaning to identify similarities. For example, a lemmatization 
# algorithm would reduce the word better to its root word, or lemme, good.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2216bb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t I \t PRON \t 4690420944186131903 \t I\n",
      "am \t am \t AUX \t 10382539506755952630 \t be\n",
      "a \t a \t DET \t 11901859001352538922 \t a\n",
      "runner \t runner \t NOUN \t 12640964157389618806 \t runner\n",
      "running \t running \t VERB \t 12767647472892411841 \t run\n",
      "in \t in \t ADP \t 3002984154512732771 \t in\n",
      "a \t a \t DET \t 11901859001352538922 \t a\n",
      "race \t race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t I \t PRON \t 4690420944186131903 \t I\n",
      "love \t love \t VERB \t 3702023516439754181 \t love\n",
      "to \t to \t PART \t 3791531372978436496 \t to\n",
      "run \t run \t VERB \t 12767647472892411841 \t run\n",
      "since \t since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t I \t PRON \t 4690420944186131903 \t I\n",
      "ran \t ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t today \t NOUN \t 11042482332948150395 \t today\n",
      ". \t . \t PUNCT \t 12646065887601541794 \t .\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "first_document = nlp(u\"I am a runner running in a race because I love to run since I ran today.\")\n",
    "for token in first_document:\n",
    "    print(token, '\\t', token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)\n",
    "    \n",
    "# token.text gives the text content of the token.\n",
    "# token.pos_ gives the part of speech for the token (Examples: verb, adjective, adverb, noun, ...)\n",
    "# token.lemma gives a number that points a specific lemma inside the loaded language library.\n",
    "# Each of the word in the language model has an individual hash to its lemma which we can reference.\n",
    "# token.lemma_ refers to the actual lemma of the token (Example: The lemma of 'am' is 'be')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed307d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "am           AUX    10382539506755952630   be\n",
      "a            DET    11901859001352538922   a\n",
      "runner       NOUN   12640964157389618806   runner\n",
      "running      VERB   12767647472892411841   run\n",
      "in           ADP    3002984154512732771    in\n",
      "a            DET    11901859001352538922   a\n",
      "race         NOUN   8048469955494714898    race\n",
      "because      SCONJ  16950148841647037698   because\n",
      "I            PRON   4690420944186131903    I\n",
      "love         VERB   3702023516439754181    love\n",
      "to           PART   3791531372978436496    to\n",
      "run          VERB   12767647472892411841   run\n",
      "since        SCONJ  10066841407251338481   since\n",
      "I            PRON   4690420944186131903    I\n",
      "ran          VERB   12767647472892411841   run\n",
      "today        NOUN   11042482332948150395   today\n",
      ".            PUNCT  12646065887601541794   .\n"
     ]
    }
   ],
   "source": [
    "def display_lemmas(text: str):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')\n",
    "\n",
    "\n",
    "display_lemmas(first_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4828769e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I\n",
      "saw          VERB   11925638236994514241   see\n",
      "10           NUM    6572986864102252890    10\n",
      "mice         NOUN   1384165645700560590    mouse\n",
      "today        NOUN   11042482332948150395   today\n",
      ".            PUNCT  12646065887601541794   .\n"
     ]
    }
   ],
   "source": [
    "second_document = nlp(u'I saw 10 mice today.')\n",
    "display_lemmas(second_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29971d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
