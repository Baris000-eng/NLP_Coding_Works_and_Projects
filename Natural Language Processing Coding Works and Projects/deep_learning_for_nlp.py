# -*- coding: utf-8 -*-
"""Deep Learning For NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14C-cd3YzluEyDjsMR81fMH68bsN8KiXF
"""

# Deep Learning For NLP

# The Basics of Perceptron Model

# Artificial Neural Networks (ANNs) have a basis in biology.

# The biological neurons have the following structure: There are several dendrites that feed into the body of the neuron cells.
# An electrical signal is passed through the dendrites. This signal goes to the cell body and is outputted through an axon.
# Then, the connection with the other neuron is performed.

# The artificial neuron has also inputs and outputs. The indexing starts at 0 for the inputs (i.e. Input 0 is the first input,
# Input 1 is the second input, and so on). The inputs of an artificial neuron will be the values of features. The inputs are
# multiplied by weights. Weights are initialized through some sort of random generation. At this step, the inputs are multiplied
# by the weights. Then, these results (the inputs multiplied by their corresponding weights) are passed to an activation function.

# There are many activation functions to choose from.

# An activation function example: If the sum of the inputs is positive, then return 1. If the sum of the inputs is negative, then
# return 0. For this activation function, we can handle the case where each of the inputs is 0 by adding a bias term to each input.
# This activation function is pretty dramatic, since the small changes are not reflected. We can have small changes in Z, for instance,
# Z from 0.6 to 0.7. It does not matter, this activation function will still output 1 as long as Z is positive. Moreover, for the
# dramatic negative changes in Z (i.e. from -1 to -1000), this activation function will output 0 for both the initial and final values.

# The mathematical representation of the perceptron model:
# Z = i from 0 to n sum(wi*xi) + b

# In the above equation; n is the number of inputs, wi is the weight specific to the input, xi is the input itself, and b is the bias term.

# The perceptron model has 2 inputs and 1 output.

# Introduction to Neural Networks

# Multiple Perceptrons Network: It includes various layers of single perceptrons connected to each other through their inputs and outputs.
# Layer Types:
# 1-) Input Layer:
  # * Input layers are the real values from the data.
# 2-) Hidden Layer
  # * Hidden layers are the layers between the input and output layers.
  # * 3 or more hidden layers are considered as a 'deep network'.
# 3-) Output Layer
  # * Output layer gives the final estimate of the output.

# As we go through more layers, the abstraction level increases.

# Abstraction in the multilayer perceptron context refers to the level of detail or complexity in the features that the neural network
# is learning to represent. In deep learning, the first layers of a neural network tend to capture low-level or basic features, while
# deeper layers capture more abstract and high-level features.

# We might need a more fine-grained activation function that is more responsive to the input changes compared to the simpler ones.
# In this context, the sigmoid function comes to the place.

# Sigmoid function ===> f(x) = 1 / (1 + e^(-x))

# Sigmoid function for the activation function ===> f(w*x+b) = 1 / (1 + e^(-(w*x+b))) = f(Z) = 1 / (1 + e^(-Z))


# Hyperbolic Tangent Activation Function (tanh or tanh(z))
# coshx = (e^x + e^(-x)) / 2
# sinhx = (e^x - e^(-x)) / 2
# tanhx = sinhx / coshx

# Hyperbolic Tantent function takes values between -1 and 1, while the sigmoid function takes values between 0 and 1.

#--------------------------------------------------------------------------------------------------------------------#
# Rectified Linear Unit (ReLU): This is a relatively simple activation function which has a formula of max(0,z).
# Please note that the 'z' in this formula is equal to w*x+b.

# ReLU tends to have the best performance in many situations.

# Softmax function is usually used at the very end of a layer in order to get some sort of
# classification output.

# Keras Basics

# Iris dataset contains measurements of flower petals and sepals, and has corresponding labels to one of three classes (3 flower species).

# Performing necessary imports
import numpy as np
from sklearn.datasets import load_iris


# Loading the iris dataset
iris = load_iris()
print(iris)
print()
print()
print()
print(type(iris))

# It gets the description of the iris dataset.
print(iris.DESCR)

X_features = iris.data
print(X_features)

y_labels = iris.target
print(y_labels)
print()
print(type(y_labels))

from keras.utils import to_categorical
print(y_labels.shape)
y_labels = to_categorical(y_labels)
print(y_labels.shape)

# divide the dataset into train-test

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, random_state = 42, test_size = 0.33)
print()
print('------------------------------------------------------------------------------------------------------')
print('The X_train dataset is as below: ')
print()
print(X_train)
print()
print('The X_test dataset is as below: ')
print()
print(X_test)
print()
print('The y_train dataset is as below: ')
print()
print(y_train)
print()
print('The y_test dataset is as below: ')
print(y_test)
print()
print('------------------------------------------------------------------------------------------------------')

# The below important statement makes sure that all values fit into the range between 0 and 1.
from sklearn.preprocessing import MinMaxScaler


lst = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]
lst = np.array(lst)

# The below division makes each element in the list called lst stay in the range from 0 to 1.
lst = lst / 50
print(lst)

scalar_object = MinMaxScaler()
scalar_object.fit(X_train) # It fits the scalar object to the X_train data.

scaled_X_train = scalar_object.transform(X_train)
scaled_X_test = scalar_object.transform(X_test)

print('The scaled X_train data is as below: ')
print()
print(scaled_X_train)
print()
print()
print()
print('The scaled X_test data is as below: ')
print()
print(scaled_X_test)
print()
print()
print('Data types of the scaled X_train and X_test data are as below: ')
print()
print(type(scaled_X_train))
print(type(scaled_X_test))

# The fact that all the values are between 0 and 1 in the train-test splits helps neural networks in the sense that
# biases and weights do not grow too large.

from keras.models import Sequential
from keras.layers import Dense

sequential_model = Sequential()

# 12 neurons, 4-dimensional input, rectified linear unit is the activation function
sequential_model.add(Dense(12, input_dim = 4, activation = 'relu'))

 # 12 neurons, 4-dimensional input, rectified linear unit is the activation function
sequential_model.add(Dense(12, input_dim = 4, activation = 'relu'))

 # Each neuron will keep the probability of belonging to a particular class, which is why we one-hat encoded.
sequential_model.add(Dense(3, activation = 'softmax'))

print(sequential_model)
print(type(sequential_model))

sequential_model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

model_summary = sequential_model.summary()
print(model_summary)

sequential_model.fit(scaled_X_train, y_train, epochs = 300, verbose = 2)

predict_x = sequential_model.predict(scaled_X_test)
print(predict_x)

predicted_classes = np.argmax(predict_x,axis=1)
print('Class assignment: ' + str(predicted_classes) + '')

actual_classes = y_test.argmax(axis=1)
print('Actual classes: '+str(actual_classes)+'')

# Model performance evaluation
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# confusion matrix
confusion_matrix = confusion_matrix(actual_classes, predicted_classes)
print(confusion_matrix)

# accuracy score
accuracy_score = accuracy_score(actual_classes, predicted_classes)
print('The overall accuracy score is: '+str(accuracy_score)+'')

# classification report
classification_report = classification_report(actual_classes, predicted_classes)
print(classification_report)

# saving the deep learning model
sequential_model.save('myfirstDLmodel.h5')

# loading the saved deep learning model
from keras.models import load_model

new_model = load_model('myfirstDLmodel.h5')
print(new_model)
print(type(new_model))

predicted_y_values = new_model.predict(scaled_X_test)
print('Predicted y values are: \n'+str(predicted_y_values)+'')
new_predicted_classes = np.argmax(predicted_y_values,axis=1)
print()
print()
print('Class assignment: \n' + str(new_predicted_classes) + '')